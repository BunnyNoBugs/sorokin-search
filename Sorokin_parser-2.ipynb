{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import html\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "session = requests.session()\n",
    "names = {'norma_part1.shtml': 'Норма', 'ochered.shtml': 'Очередь', \n",
    "         'marina_part1.shtml': 'Тридацатая любовь Марины', 'roman_part0.shtml': 'Роман',\n",
    "        'serdtsa4_part01.shtml': 'Сердца четырех', 'salo_part1.shtml': 'Голубое сало',\n",
    "        'led_part01.shtml': 'Лед', 'bro_part01.shtml': 'Путь Бро', \n",
    "        '23000_part08.html': '23000 главы из романа', 'telluria.html': 'Теллурия глава из романа'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала напишем функцию, которая будет очищать извлечённый текст от тэгов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    regex_tag = re.compile('<.*?>', re.DOTALL)\n",
    "    regex_tag_t = re.compile('{.*?}', re.DOTALL)\n",
    "    regex_space = re.compile('\\s{2,}', re.DOTALL)\n",
    "    clean_t = regex_space.sub('', text)\n",
    "    clean_t = regex_tag_t.sub ('', clean_t)\n",
    "    clean_t = regex_tag.sub('', clean_t)\n",
    "    clean_t = html.unescape(clean_t)\n",
    "    return clean_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем функцию, которая со страницы \"Романы\" на сайте Сорокина будет получать ссылки на \"стартовые страницы\" романов и возвращать список ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(genre):\n",
    "    req = session.get('https://srkn.ru/' + genre, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    novel_links = soup.find_all('span', {'class': 'field-content'})\n",
    "    links_start = []\n",
    "    for link in novel_links:\n",
    "        new = re.findall('href=\"texts/(.*).?html', str(link))\n",
    "        new = str(new[0]) + 'html'\n",
    "        links_start.append(new)\n",
    "    return links_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем функцию, которая берёт список ссылок и проходится со стартовой ссылки до конца романа через page-next. Каждая страница добавляется в один общий список texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(links_start):\n",
    "    texts = []\n",
    "    for num, l in enumerate(links_start):\n",
    "        pages = ''\n",
    "        nex = l\n",
    "        while len(nex)!=0:\n",
    "            req = session.get('https://srkn.ru/texts/' + nex, headers={'User-Agent': ua.random})\n",
    "            page = req.text\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            text_pro = soup.find_all('div', {'class': 'field-item even'})\n",
    "            #Сайт Сорокина устроен так, что либо текст идёт четвертым в div class field-item even, либо последним\n",
    "            #Последним, элементов меньше 4.\n",
    "            if len(text_pro) > 3:\n",
    "                new_text = clean_text(str(text_pro[3]))\n",
    "            else:\n",
    "                new_text = clean_text(str(text_pro[-1]))\n",
    "            pages = pages + new_text\n",
    "            links = soup.find_all('a', {'class': 'page-next'})\n",
    "            nex = re.findall('href=\"/texts/(.*).?html', str(links))\n",
    "            if len(nex) != 0:\n",
    "                nex = str(nex[0]) + 'html'\n",
    "            \n",
    "        title = names[l]\n",
    "        to_tuple = (title, pages)\n",
    "        texts.append(to_tuple)\n",
    "        \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и наконец запись в файл и вызов функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_romany = get_names('romany')\n",
    "texts_romany = get_texts(links_romany)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts_romany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sorokin_corpus.txt', 'w') as f:\n",
    "    for text in texts:\n",
    "        f.write(text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сразу на основе текстов сделаем модель markovify и сохраним её."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = json.dumps(texts_romany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(texts_romany, f, ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
